"""
CryptoAce æ™ºèƒ½äº¤æ˜“ä»£ç†æ¨¡çµ„

æ­¤æ¨¡çµ„å¯¦ç¾åŸºæ–¼ PPO å’Œ Transformer çš„äº¤æ˜“ä»£ç†ï¼Œæ•´åˆï¼š
- Stable-Baselines3 PPO æ¨¡å‹
- è‡ªå®šç¾© Transformer ç‰¹å¾µæå–å™¨
- å®Œæ•´çš„è¨“ç·´å’Œé æ¸¬åŠŸèƒ½
"""

import os
import numpy as np
import torch
import torch.nn as nn
from typing import Any, Dict, Optional, Tuple, Union
import gymnasium as gym
from pathlib import Path
import warnings

# Stable-Baselines3 ç›¸é—œå°å…¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.policies import ActorCriticPolicy
    from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
    from stable_baselines3.common.callbacks import BaseCallback
    from stable_baselines3.common.vec_env import DummyVecEnv
    SB3_AVAILABLE = True
except ImportError:
    print("Warning: stable-baselines3 not available. Agent functionality will be limited.")
    SB3_AVAILABLE = False
    # å‰µå»ºè™›æ“¬é¡åˆ¥ä»¥é¿å…å°å…¥éŒ¯èª¤
    class BaseFeaturesExtractor:
        pass
    class BaseCallback:
        pass

# è™•ç†ç›¸å°åŒ¯å…¥å•é¡Œ
try:
    from .interfaces import IAgent
    from .configurator import Configurator
    from .logger import setup_logger
    from .trading_env import TradingEnv
except ImportError:
    # ç•¶ç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆæ™‚ï¼Œä½¿ç”¨çµ•å°åŒ¯å…¥
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core.interfaces import IAgent
    from core.configurator import Configurator
    from core.logger import setup_logger
    from core.trading_env import TradingEnv

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')


class TransformerExtractor(BaseFeaturesExtractor):
    """
    è‡ªå®šç¾© Transformer ç‰¹å¾µæå–å™¨
    
    å¯¦ç¾åŸºæ–¼ Transformer ç·¨ç¢¼å™¨çš„ç‰¹å¾µæå–ï¼Œé©ç”¨æ–¼æ™‚åºé‡‘èæ•¸æ“š
    """
    
    def __init__(
        self,
        observation_space: gym.Space,
        features_dim: int = 256,
        n_heads: int = 8,
        n_layers: int = 4,
        dropout: float = 0.1,
        sequence_length: int = 50
    ):
        """
        åˆå§‹åŒ– Transformer ç‰¹å¾µæå–å™¨
        
        Args:
            observation_space: è§€å¯Ÿç©ºé–“
            features_dim: è¼¸å‡ºç‰¹å¾µç¶­åº¦
            n_heads: æ³¨æ„åŠ›é ­æ•¸
            n_layers: Transformer å±¤æ•¸
            dropout: Dropout æ¯”ä¾‹
            sequence_length: åºåˆ—é•·åº¦ï¼ˆç”¨æ–¼ä½ç½®ç·¨ç¢¼ï¼‰
        """
        super().__init__(observation_space, features_dim)
        
        if not SB3_AVAILABLE:
            raise ImportError("stable-baselines3 is required for TransformerExtractor")
        
        # ç²å–è¼¸å…¥ç¶­åº¦
        self.input_dim = observation_space.shape[0]
        self._features_dim = features_dim  # ä½¿ç”¨ç§æœ‰å±¬æ€§é¿å…èˆ‡çˆ¶é¡è¡çª
        self.sequence_length = sequence_length
        
        # è¼¸å…¥æŠ•å½±å±¤
        self.input_projection = nn.Linear(self.input_dim, self._features_dim)
        
        # ä½ç½®ç·¨ç¢¼
        self.positional_encoding = self._create_positional_encoding(sequence_length, self._features_dim)
        
        # Transformer ç·¨ç¢¼å™¨å±¤
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self._features_dim,
            nhead=n_heads,
            dim_feedforward=self._features_dim * 4,
            dropout=dropout,
            activation='relu',
            batch_first=True
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_layers
        )
        
        # è¼¸å‡ºæŠ•å½±å±¤
        self.output_projection = nn.Sequential(
            nn.Linear(self._features_dim, self._features_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self._features_dim, self._features_dim)
        )
        
        # Layer Normalization
        self.layer_norm = nn.LayerNorm(self._features_dim)
        
    def _create_positional_encoding(self, max_length: int, d_model: int) -> torch.Tensor:
        """å‰µå»ºä½ç½®ç·¨ç¢¼"""
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # [1, max_length, d_model]
        
        return pe
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            observations: è§€å¯Ÿæ•¸æ“š [batch_size, input_dim]
            
        Returns:
            æå–çš„ç‰¹å¾µ [batch_size, features_dim]
        """
        batch_size = observations.shape[0]
        
        # è¼¸å…¥æŠ•å½±
        x = self.input_projection(observations)  # [batch_size, _features_dim]
        
        # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦ä»¥é©æ‡‰åºåˆ—è™•ç†
        # é€™è£¡æˆ‘å€‘å°‡å–®å€‹è§€å¯Ÿè¦–ç‚ºé•·åº¦ç‚º1çš„åºåˆ—
        x = x.unsqueeze(1)  # [batch_size, 1, _features_dim]
        
        # æ·»åŠ ä½ç½®ç·¨ç¢¼ï¼ˆåªä½¿ç”¨ç¬¬ä¸€å€‹ä½ç½®ï¼‰
        if self.positional_encoding.device != x.device:
            self.positional_encoding = self.positional_encoding.to(x.device)
        
        pos_encoding = self.positional_encoding[:, :1, :]  # [1, 1, _features_dim]
        x = x + pos_encoding
        
        # Transformer ç·¨ç¢¼
        x = self.transformer_encoder(x)  # [batch_size, 1, _features_dim]
        
        # ç§»é™¤åºåˆ—ç¶­åº¦
        x = x.squeeze(1)  # [batch_size, _features_dim]
        
        # å±¤æ­¸ä¸€åŒ–å’Œè¼¸å‡ºæŠ•å½±
        x = self.layer_norm(x)
        x = self.output_projection(x)
        
        return x


class TrainingCallback(BaseCallback):
    """
    è¨“ç·´å›èª¿å‡½æ•¸
    
    ç”¨æ–¼ç›£æ§è¨“ç·´éç¨‹å’Œè¨˜éŒ„æ—¥èªŒ
    """
    
    def __init__(self, logger=None, log_interval: int = 1000):
        super().__init__()
        self._logger = logger  # ä½¿ç”¨ç§æœ‰å±¬æ€§å­˜å„² logger
        self.log_interval = log_interval
        self.episode_rewards = []
        self.episode_lengths = []

    @property
    def logger(self):
        return self._logger

    @logger.setter
    def logger(self, value):
        self._logger = value

    def _on_step(self) -> bool:
        """æ¯æ­¥èª¿ç”¨çš„å›èª¿"""
        # è¨˜éŒ„å›åˆçµæŸä¿¡æ¯
        if len(self.locals.get('dones', [])) > 0 and any(self.locals['dones']):
            if len(self.locals.get('infos', [])) > 0:
                for info in self.locals['infos']:
                    if 'episode' in info:
                        self.episode_rewards.append(info['episode']['r'])
                        self.episode_lengths.append(info['episode']['l'])
        
        # å®šæœŸæ—¥èªŒè¨˜éŒ„
        if self.num_timesteps % self.log_interval == 0:
            if self.logger and len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards[-10:])  # æœ€è¿‘10å€‹å›åˆçš„å¹³å‡çå‹µ
                mean_length = np.mean(self.episode_lengths[-10:])  # æœ€è¿‘10å€‹å›åˆçš„å¹³å‡é•·åº¦
                
                self.logger.info(f"Step {self.num_timesteps}: "
                               f"Mean Reward: {mean_reward:.4f}, "
                               f"Mean Length: {mean_length:.1f}")
        
        return True


class Agent(IAgent):
    """
    æ™ºèƒ½äº¤æ˜“ä»£ç†
    
    åŸºæ–¼ PPO å’Œ Transformer çš„å¼·åŒ–å­¸ç¿’äº¤æ˜“ä»£ç†
    """
    
    def __init__(
        self,
        env: gym.Env,
        config: Optional[Configurator] = None,
        logger: Optional[Any] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–ä»£ç†
        
        Args:
            env: äº¤æ˜“ç’°å¢ƒ
            config: é…ç½®ç®¡ç†å™¨
            logger: æ—¥èªŒè¨˜éŒ„å™¨
            **kwargs: é¡å¤–åƒæ•¸
        """
        if not SB3_AVAILABLE:
            raise ImportError("stable-baselines3 is required for Agent")
        
        self.env = env
        self.config = config
        
        # è¨­ç½®æ—¥èªŒè¨˜éŒ„å™¨
        if logger:
            self.logger = logger
        elif config and hasattr(config, 'logger'):
            self.logger = setup_logger(config)
        else:
            self.logger = None
        
        # å¾é…ç½®ä¸­ç²å–åƒæ•¸
        if config:
            agent_config = config.agent
            model_config = getattr(config, 'model', {})
        else:
            agent_config = {}
            model_config = {}
        
        # PPO åƒæ•¸
        self.learning_rate = agent_config.get('learning_rate', 3e-4)
        self.batch_size = agent_config.get('batch_size', 64)
        self.n_epochs = agent_config.get('n_epochs', 10)
        self.gamma = agent_config.get('gamma', 0.99)
        self.gae_lambda = agent_config.get('gae_lambda', 0.95)
        self.clip_range = agent_config.get('clip_range', 0.2)
        self.ent_coef = agent_config.get('ent_coef', 0.01)
        self.vf_coef = agent_config.get('vf_coef', 0.5)
        self.max_grad_norm = agent_config.get('max_grad_norm', 0.5)
        
        # Transformer åƒæ•¸
        self.features_dim = model_config.get('features_dim', 256)
        self.n_heads = model_config.get('n_heads', 8)
        self.n_layers = model_config.get('n_layers', 4)
        self.dropout = model_config.get('dropout', 0.1)
        
        # è¨­å‚™é¸æ“‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å‰µå»ºè‡ªå®šç¾©ç­–ç•¥åƒæ•¸
        policy_kwargs = {
            "features_extractor_class": TransformerExtractor,
            "features_extractor_kwargs": {
                "features_dim": self.features_dim,
                "n_heads": self.n_heads,
                "n_layers": self.n_layers,
                "dropout": self.dropout
            },
            "net_arch": [dict(pi=[256, 256], vf=[256, 256])],  # ç­–ç•¥å’Œåƒ¹å€¼ç¶²è·¯æ¶æ§‹
            "activation_fn": torch.nn.ReLU,
        }
        
        # å‰µå»º PPO æ¨¡å‹
        self.model = PPO(
            policy="MlpPolicy",
            env=env,
            learning_rate=self.learning_rate,
            n_steps=2048,  # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•¸
            batch_size=self.batch_size,
            n_epochs=self.n_epochs,
            gamma=self.gamma,
            gae_lambda=self.gae_lambda,
            clip_range=self.clip_range,
            ent_coef=self.ent_coef,
            vf_coef=self.vf_coef,
            max_grad_norm=self.max_grad_norm,
            policy_kwargs=policy_kwargs,
            device=self.device,
            verbose=1 if self.logger else 0
        )
        
        # è¨“ç·´å›èª¿
        self.callback = TrainingCallback(logger=self.logger)
        
        if self.logger:
            self.logger.info(f"Agent åˆå§‹åŒ–å®Œæˆ")
            self.logger.info(f"  - è¨­å‚™: {self.device}")
            self.logger.info(f"  - å­¸ç¿’ç‡: {self.learning_rate}")
            self.logger.info(f"  - æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            self.logger.info(f"  - Transformer ç‰¹å¾µç¶­åº¦: {self.features_dim}")
            self.logger.info(f"  - æ³¨æ„åŠ›é ­æ•¸: {self.n_heads}")
            self.logger.info(f"  - Transformer å±¤æ•¸: {self.n_layers}")
    
    def train(
        self,
        total_timesteps: int,
        save_path: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è¨“ç·´ä»£ç†
        
        Args:
            total_timesteps: ç¸½è¨“ç·´æ­¥æ•¸
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾‘
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            è¨“ç·´çµæœå­—å…¸
        """
        if self.logger:
            self.logger.info(f"é–‹å§‹è¨“ç·´ä»£ç†ï¼Œç¸½æ­¥æ•¸: {total_timesteps}")
        
        try:
            # é–‹å§‹è¨“ç·´
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=self.callback,
                progress_bar=True,
                **kwargs
            )
            
            # ä¿å­˜æ¨¡å‹
            if save_path:
                self.save(save_path)
                if self.logger:
                    self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            
            # æ”¶é›†è¨“ç·´çµ±è¨ˆä¿¡æ¯
            training_stats = {
                'total_timesteps': total_timesteps,
                'episode_rewards': self.callback.episode_rewards.copy(),
                'episode_lengths': self.callback.episode_lengths.copy(),
                'mean_reward': np.mean(self.callback.episode_rewards[-10:]) if self.callback.episode_rewards else 0,
                'std_reward': np.std(self.callback.episode_rewards[-10:]) if self.callback.episode_rewards else 0,
            }
            
            if self.logger:
                self.logger.info(f"è¨“ç·´å®Œæˆï¼å¹³å‡çå‹µ: {training_stats['mean_reward']:.4f}")
            
            return training_stats
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            raise
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = True
    ) -> Tuple[np.ndarray, Optional[Dict[str, Any]]]:
        """
        é æ¸¬å‹•ä½œ
        
        Args:
            observation: è§€å¯Ÿç‹€æ…‹
            deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥
            
        Returns:
            (action, extra_info): å‹•ä½œå’Œé¡å¤–ä¿¡æ¯
        """
        try:
            # å°‡è§€å¯Ÿè½‰æ›ç‚º torch.Tensor
            obs_tensor = torch.tensor(observation, dtype=torch.float32, device=self.device)
            
            action, state = self.model.predict(
                observation=obs_tensor,
                deterministic=deterministic
            )
            
            # é¡å¤–ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            extra_info = {
                'state': state,
                'deterministic': deterministic
            }
            
            return action, extra_info
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"é æ¸¬éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            raise
    
    def save(self, path: str) -> None:
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            path: ä¿å­˜è·¯å¾‘
        """
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            save_path = Path(path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ PPO æ¨¡å‹
            self.model.save(str(save_path))
            
            if self.logger:
                self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"ä¿å­˜æ¨¡å‹æ™‚å‡ºç¾éŒ¯èª¤: {e}")
            raise
    
    def load(self, path: str) -> None:
        """
        åŠ è¼‰æ¨¡å‹
        
        Args:
            path: æ¨¡å‹è·¯å¾‘
        """
        try:
            if not os.path.exists(path + ".zip"):  # PPO è‡ªå‹•æ·»åŠ  .zip å¾Œç¶´
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            
            # åŠ è¼‰ PPO æ¨¡å‹
            self.model = PPO.load(
                path=path,
                env=self.env,
                device=self.device
            )
            
            if self.logger:
                self.logger.info(f"æ¨¡å‹å·²å¾ {path} åŠ è¼‰")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"åŠ è¼‰æ¨¡å‹æ™‚å‡ºç¾éŒ¯èª¤: {e}")
            raise
    
    def evaluate(
        self,
        n_episodes: int = 10,
        deterministic: bool = True
    ) -> Dict[str, float]:
        """
        è©•ä¼°ä»£ç†æ€§èƒ½
        
        Args:
            n_episodes: è©•ä¼°å›åˆæ•¸
            deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥
            
        Returns:
            è©•ä¼°çµæœå­—å…¸
        """
        if self.logger:
            self.logger.info(f"é–‹å§‹è©•ä¼°ä»£ç†ï¼Œå›åˆæ•¸: {n_episodes}")
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(n_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            terminated = False
            truncated = False
            
            while not (terminated or truncated):
                action, _ = self.predict(obs, deterministic=deterministic)
                obs, reward, terminated, truncated, info = self.env.step(action)
                episode_reward += reward
                episode_length += 1
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if self.logger and (episode + 1) % 5 == 0:
                self.logger.info(f"è©•ä¼°é€²åº¦: {episode + 1}/{n_episodes}, "
                               f"ç•¶å‰å›åˆçå‹µ: {episode_reward:.4f}")
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        eval_stats = {
            'mean_reward': float(np.mean(episode_rewards)),
            'std_reward': float(np.std(episode_rewards)),
            'min_reward': float(np.min(episode_rewards)),
            'max_reward': float(np.max(episode_rewards)),
            'mean_length': float(np.mean(episode_lengths)),
            'std_length': float(np.std(episode_lengths)),
        }
        
        if self.logger:
            self.logger.info(f"è©•ä¼°å®Œæˆï¼å¹³å‡çå‹µ: {eval_stats['mean_reward']:.4f} Â± {eval_stats['std_reward']:.4f}")
        
        return eval_stats


if __name__ == "__main__":
    """æ¸¬è©¦ Agent æ¨¡çµ„"""
    import pandas as pd
    from pathlib import Path
    
    print("=== CryptoAce Agent æ¸¬è©¦ ===")
    
    # æª¢æŸ¥ stable-baselines3 å¯ç”¨æ€§
    if not SB3_AVAILABLE:
        print("âŒ stable-baselines3 ä¸å¯ç”¨ï¼Œç„¡æ³•é€²è¡Œå®Œæ•´æ¸¬è©¦")
        print("è«‹å®‰è£: pip install stable-baselines3")
        exit(1)
    
    try:
        # 1. å‰µå»ºè™›æ“¬äº¤æ˜“ç’°å¢ƒ
        print("\n1. å‰µå»ºè™›æ“¬äº¤æ˜“ç’°å¢ƒ...")
        
        # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=200, freq='1h')
        
        # å‰µå»ºåƒ¹æ ¼æ•¸æ“š
        base_price = 50000.0
        prices = [base_price]
        for i in range(1, 200):
            change = np.random.normal(0, 0.02)
            new_price = prices[-1] * (1 + change)
            prices.append(new_price)
        
        # å‰µå»º OHLCV æ•¸æ“š
        data_list = []
        for i, (date, close) in enumerate(zip(dates, prices)):
            high = close * (1 + abs(np.random.normal(0, 0.01)))
            low = close * (1 - abs(np.random.normal(0, 0.01)))
            open_price = prices[i-1] if i > 0 else close
            volume = np.random.uniform(1000, 5000)
            
            data_list.append({
                'timestamp': date,
                'open': open_price,
                'high': high,
                'low': low,
                'close': close,
                'volume': volume
            })
        
        data = pd.DataFrame(data_list)
        data.set_index('timestamp', inplace=True)
        
        # æ·»åŠ ç‰¹å¾µ
        data['returns'] = data['close'].pct_change().fillna(0)
        data['sma_10'] = data['close'].rolling(10).mean()
        data['volatility'] = data['returns'].rolling(10).std()
        
        for i in range(5):
            data[f'feature_{i}'] = np.random.randn(len(data))
        
        data = data.dropna()
        
        print(f"   æ•¸æ“šå½¢ç‹€: {data.shape}")
        
        # å‰µå»ºäº¤æ˜“ç’°å¢ƒ
        env = TradingEnv(
            data=data,
            initial_balance=100000.0,
            transaction_cost=0.001,
            max_position_change_per_step=0.1,
            max_drawdown_limit=0.3,
            lookback_window=20
        )
        
        print(f"   ç’°å¢ƒè§€å¯Ÿç©ºé–“: {env.observation_space}")
        print(f"   ç’°å¢ƒå‹•ä½œç©ºé–“: {env.action_space}")
        
        # 2. å¯¦ä¾‹åŒ– Agent
        print("\n2. å¯¦ä¾‹åŒ– Agent...")
        
        agent = Agent(
            env=env,
            config=None  # ä½¿ç”¨é»˜èªé…ç½®
        )
        
        print(f"   Agent è¨­å‚™: {agent.device}")
        print(f"   Transformer ç‰¹å¾µç¶­åº¦: {agent.features_dim}")
        print(f"   æ³¨æ„åŠ›é ­æ•¸: {agent.n_heads}")
        
        # 3. æ¸¬è©¦é æ¸¬åŠŸèƒ½
        print("\n3. æ¸¬è©¦é æ¸¬åŠŸèƒ½...")
        
        obs, info = env.reset(seed=42)
        print(f"   åˆå§‹è§€å¯Ÿå½¢ç‹€: {obs.shape}")
        
        # æ¸¬è©¦é æ¸¬
        action, extra_info = agent.predict(obs, deterministic=True)
        print(f"   é æ¸¬å‹•ä½œ: {action}")
        print(f"   å‹•ä½œå½¢ç‹€: {action.shape}")
        print(f"   æ˜¯å¦ç¢ºå®šæ€§: {extra_info['deterministic']}")
        
        # æ¸¬è©¦éš¨æ©Ÿé æ¸¬
        action_random, _ = agent.predict(obs, deterministic=False)
        print(f"   éš¨æ©Ÿå‹•ä½œ: {action_random}")
        
        # 4. æ¸¬è©¦ç°¡çŸ­è¨“ç·´
        print("\n4. æ¸¬è©¦ç°¡çŸ­è¨“ç·´...")
        
        # å¾ˆçŸ­çš„è¨“ç·´æ¸¬è©¦ï¼ˆåªæ˜¯é©—è­‰èƒ½é‹è¡Œï¼‰
        training_stats = agent.train(total_timesteps=1000)
        print(f"   è¨“ç·´å®Œæˆï¼Œç¸½æ­¥æ•¸: {training_stats['total_timesteps']}")
        print(f"   å›åˆæ•¸: {len(training_stats['episode_rewards'])}")
        if training_stats['episode_rewards']:
            print(f"   å¹³å‡çå‹µ: {training_stats['mean_reward']:.4f}")
        
        # 5. æ¸¬è©¦ä¿å­˜å’Œè¼‰å…¥
        print("\n5. æ¸¬è©¦ä¿å­˜å’Œè¼‰å…¥...")
        
        # æ¸¬è©¦ä¿å­˜
        save_path = "./models/test_agent"
        agent.save(save_path)
        print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        # é©—è­‰æ–‡ä»¶å­˜åœ¨
        if os.path.exists(save_path + ".zip"):
            print("   âœ… ä¿å­˜æ–‡ä»¶å­˜åœ¨")
        else:
            print("   âŒ ä¿å­˜æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ¸¬è©¦è¼‰å…¥
        try:
            # å‰µå»ºæ–°çš„ agent å¯¦ä¾‹ä¾†æ¸¬è©¦è¼‰å…¥
            new_agent = Agent(env=env, config=None)
            new_agent.load(save_path)
            print("   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            
            # æ¸¬è©¦è¼‰å…¥å¾Œçš„é æ¸¬
            action_loaded, _ = new_agent.predict(obs, deterministic=True)
            print(f"   è¼‰å…¥å¾Œé æ¸¬: {action_loaded}")
            
        except Exception as e:
            print(f"   âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        
        # 6. æ¸¬è©¦è©•ä¼°åŠŸèƒ½
        print("\n6. æ¸¬è©¦è©•ä¼°åŠŸèƒ½...")
        
        eval_stats = agent.evaluate(n_episodes=3, deterministic=True)
        print(f"   è©•ä¼°å®Œæˆ:")
        print(f"     å¹³å‡çå‹µ: {eval_stats['mean_reward']:.4f}")
        print(f"     æ¨™æº–å·®: {eval_stats['std_reward']:.4f}")
        print(f"     å¹³å‡é•·åº¦: {eval_stats['mean_length']:.1f}")
        
        print("\nâœ… Agent æ¸¬è©¦å®Œæˆï¼")
        print("\nğŸ¯ æ ¸å¿ƒåŠŸèƒ½é©—è­‰:")
        print("  âœ“ Transformer ç‰¹å¾µæå–å™¨")
        print("  âœ“ PPO æ¨¡å‹æ•´åˆ")
        print("  âœ“ é æ¸¬åŠŸèƒ½")
        print("  âœ“ è¨“ç·´åŠŸèƒ½")
        print("  âœ“ ä¿å­˜å’Œè¼‰å…¥")
        print("  âœ“ è©•ä¼°åŠŸèƒ½")
        
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        try:
            if os.path.exists(save_path + ".zip"):
                os.remove(save_path + ".zip")
                print("\nğŸ§¹ æ¸¬è©¦æ–‡ä»¶å·²æ¸…ç†")
        except:
            pass
        
    except Exception as e:
        print(f"âŒ Agent æ¸¬è©¦å¤±æ•—: {e}")
        print("\néŒ¯èª¤è©³æƒ…:")
        import traceback
        traceback.print_exc()
